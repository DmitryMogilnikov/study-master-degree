{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from jiwer import RemoveWhiteSpace, ReduceToListOfListOfChars, Compose, wer, cer\n",
    "from datasets import Dataset, Audio\n",
    "from torchvision.datasets import ImageFolder\n",
    "from transformers import ( Wav2Vec2CTCTokenizer, \n",
    "                           Wav2Vec2FeatureExtractor, \n",
    "                           Wav2Vec2Processor, \n",
    "                           Wav2Vec2ForCTC\n",
    "                        )\n",
    "\n",
    "from typing import Generator, Any"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сравнение результатов моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_alphanum_chars_regexp = re.compile(r\"[^\\w\\s]\", flags=re.IGNORECASE)\n",
    "\n",
    "def remove_special_characters(text: str) -> str:\n",
    "    return re.sub(non_alphanum_chars_regexp, '', text).upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples_generator(lang: str) -> Generator[dict[str, Any], None, None]:\n",
    "    base_path = Path(f\"./data/test_audio/{lang}/\")\n",
    "    file_path_patttern = f\"*.wav\"\n",
    "    for path in base_path.glob(file_path_patttern):\n",
    "        file_path_template = f\"{path.parent.name}/{path.stem}.{{ext}}\"\n",
    "\n",
    "        audio_path = f\"./data/test_audio/{file_path_template.format(ext='wav')}\"\n",
    "        text_path = f\"./data/test_transcription/{file_path_template.format(ext='txt')}\"\n",
    "\n",
    "        with open(text_path, \"r\") as f:\n",
    "            transcription = f.read()\n",
    "        transcription = remove_special_characters(transcription)\n",
    "\n",
    "        yield { \"audio\": audio_path, \"transcription\": transcription }\n",
    "\n",
    "def load_dataset(lang: str) -> Dataset:\n",
    "    return Dataset.from_generator(samples_generator, gen_kwargs={\"lang\": lang}).cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_processor(tokenizer_type: str) -> Wav2Vec2Processor:\n",
    "    if tokenizer_type == \"en\":\n",
    "        from transformers import AutoProcessor\n",
    "        return AutoProcessor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "\n",
    "    tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(f\"./models/{tokenizer_type}/\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\" \")\n",
    "    feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "    return Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset: Dataset, processor: Wav2Vec2Processor) -> Dataset:\n",
    "    \n",
    "    def prepare_dataset(batch):\n",
    "        audio = batch[\"audio\"]\n",
    "        batch = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], text=batch[\"transcription\"])\n",
    "        batch[\"input_length\"] = len(batch[\"input_values\"][0])\n",
    "        return batch\n",
    "    \n",
    "    return dataset.map(prepare_dataset, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_and_processor(lang: str) -> (Dataset, Wav2Vec2Processor):\n",
    "    dataset = load_dataset(lang)\n",
    "    processor = create_processor(lang)\n",
    "    dataset = tokenize_dataset(dataset, processor)\n",
    "    return dataset, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_state_path: str, processor: Wav2Vec2Processor, model_type: str = \"facebook/wav2vec2-base\") -> Wav2Vec2ForCTC:\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\n",
    "        model_type,\n",
    "        pad_token_id=processor.tokenizer.pad_token_id, \n",
    "        vocab_size=len(processor.tokenizer)\n",
    "    )\n",
    "    model.load_state_dict(torch.load(model_state_path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets_processors_models(model_paths: dict[str, str]) -> (dict[str, Dataset], dict[str, Wav2Vec2Processor], dict[str, Wav2Vec2ForCTC]):\n",
    "    datasets, processors, models = {}, {}, {}\n",
    "    for lang in [\"en\", \"ru\", \"de\"]:\n",
    "        datasets[lang], processor = create_dataset_and_processor(lang)\n",
    "        processors[lang] = processor\n",
    "        models[lang] = load_model(model_paths[lang], processor)\n",
    "    return datasets, processors, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=5, stride=1, padding = 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.fc1 = nn.Linear(18432, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lid_model_path = \"./models/lid/lid_model_state.pth\"\n",
    "\n",
    "model_paths = {\n",
    "    \"en\": \"./models/en/en_model_state.pth\",\n",
    "    \"ru\": \"./models/ru/ru_model_state.pth\",\n",
    "    \"de\": \"./models/de/de_model_state.pth\"\n",
    "}\n",
    "\n",
    "multilingual_model_path = \"./models/multilingual/multilingual_model_state.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lid_dataset = ImageFolder(root=\"./data/test_spectrogram\", transform=torchvision.transforms.ToTensor())\n",
    "lid_model = CNNModel()\n",
    "lid_model.load_state_dict(torch.load(lid_model_path))\n",
    "\n",
    "datasets, processors, models = load_datasets_processors_models(model_paths)\n",
    "\n",
    "multilingual_processor = create_processor(\"multilingual\")\n",
    "multilingual_model = load_model(multilingual_model_path, multilingual_processor, \"facebook/wav2vec2-xls-r-300m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}.\")\n",
    "\n",
    "lid_model = lid_model.to(device)\n",
    "for key, model in models.items():\n",
    "    models[key] = model.to(device)\n",
    "multilingual_model = multilingual_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EvaluationPipelineResult:\n",
    "    predicted_class: str\n",
    "    true_class: str\n",
    "    single_model_result: str\n",
    "    multi_model_result: str\n",
    "    reference_result: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_by_lid_index(image_index: int, true_class: str) -> int:\n",
    "    if true_class in [\"en\", \"ru\"]:\n",
    "        image_index -= len(datasets[\"de\"])\n",
    "    if true_class == \"ru\":\n",
    "        image_index -= len(datasets[\"en\"])\n",
    "    return image_index\n",
    "\n",
    "def predict(model: Wav2Vec2ForCTC, processor: Wav2Vec2Processor, entry) -> (torch.Tensor, str):\n",
    "    input_dict = processor(entry[\"input_values\"], sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    input_dict = {k: v.to(device) for k, v in input_dict.items()}\n",
    "\n",
    "    logits = model(**input_dict).logits\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    return processor.batch_decode(pred_ids)[0]\n",
    "\n",
    "def evaluate_predict_pipeline(lid_sample_idx: int) -> EvaluationPipelineResult:\n",
    "    image, true_class_idx = lid_dataset[lid_sample_idx]\n",
    "\n",
    "    image = image[None,:].to(device)\n",
    "    lid_logits = lid_model(image)\n",
    "    predicted_class_idx = torch.argmax(lid_logits, dim=-1)\n",
    "\n",
    "    true_class = lid_dataset.classes[true_class_idx]\n",
    "    predicted_class = lid_dataset.classes[predicted_class_idx]\n",
    "\n",
    "    model = models[predicted_class]\n",
    "    processor = processors[predicted_class]\n",
    "    true_processor = processors[true_class]\n",
    "    dataset = datasets[true_class]\n",
    "\n",
    "    index = get_index_by_lid_index(lid_sample_idx, true_class)\n",
    "    entry = dataset[index]\n",
    "\n",
    "    single_language_prediction = predict(model, processor, entry)\n",
    "    multi_language_prediction = predict(multilingual_model, multilingual_processor, entry)\n",
    "\n",
    "    reference = true_processor.decode(entry[\"labels\"])\n",
    "\n",
    "    return EvaluationPipelineResult(predicted_class, true_class, single_language_prediction, multi_language_prediction, reference)\n",
    "\n",
    "def print_prediction():\n",
    "    with torch.no_grad():\n",
    "        rand_int = random.randint(0, len(lid_dataset) - 1)\n",
    "        result = evaluate_predict_pipeline(rand_int)\n",
    "        \n",
    "        print(f\"Predicted class: {result.predicted_class}, true class: {result.true_class}.\")\n",
    "        print(f\"Prediction (single language): {result.single_model_result}\")\n",
    "        print(f\"Prediction (multilingual): {result.multi_model_result}\")\n",
    "        print(f\"Reference: {result.reference_result}\")\n",
    "\n",
    "def print_metrics():\n",
    "    with torch.no_grad():\n",
    "        single_wer = 0\n",
    "        single_cer = 0\n",
    "        single_nospace_cer = 0\n",
    "        multi_wer = 0\n",
    "        multi_cer = 0\n",
    "        multi_nospace_cer = 0\n",
    "\n",
    "        transform = Compose([\n",
    "            RemoveWhiteSpace(),\n",
    "            ReduceToListOfListOfChars()\n",
    "        ])\n",
    "        samples_count = len(lid_dataset)\n",
    "        \n",
    "        for lid_index in range(samples_count):\n",
    "            result = evaluate_predict_pipeline(lid_index)\n",
    "            ref = result.reference_result\n",
    "            single_res = result.single_model_result\n",
    "            multi_res = result.multi_model_result\n",
    "\n",
    "            single_wer += wer(ref, single_res)\n",
    "            single_cer += cer(ref, single_res)\n",
    "            single_nospace_cer += cer(ref, single_res, transform, transform)\n",
    "\n",
    "            multi_wer += wer(ref, multi_res)\n",
    "            multi_cer += cer(ref, multi_res)\n",
    "            multi_nospace_cer += cer(ref, multi_res, transform, transform)\n",
    "\n",
    "        single_wer /= samples_count\n",
    "        single_cer /= samples_count\n",
    "        single_nospace_cer /= samples_count\n",
    "        multi_wer /= samples_count\n",
    "        multi_cer /= samples_count\n",
    "        multi_nospace_cer /= samples_count\n",
    "\n",
    "        print(f\"Single language: wer = {single_wer:.4f}, cer = {single_cer:.4f}, cer (ignoring spaces) = {single_nospace_cer:.4f}\")\n",
    "        print(f\"Multilingual: wer = {multi_wer:.4f}, cer = {multi_cer:.4f}, cer (ignoring spaces) = {multi_nospace_cer:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single language: wer = 0.8780, cer = 0.5066, cer (ignoring spaces) = 0.5285\n",
      "Multilingual: wer = 1.0967, cer = 0.9592, cer (ignoring spaces) = 1.0677\n"
     ]
    }
   ],
   "source": [
    "print_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: en, true class: en.\n",
      "Prediction (single language): CALIN ABOUT  REINT TRANACTIFOM MY CARD\n",
      "Prediction (multilingual): olihebo reez fant twinboc onl my card\n",
      "Reference: TEL ME ABOUT RECENT TRANSACTIONS ON MY CARD\n",
      "\n",
      "Predicted class: de, true class: en.\n",
      "Prediction (single language): UTZ MUSTE MANE MUNE ERKENUGE DRAGEN WON TZIEN INND DUNGIN TEN TAEGE DEL FR MINNITIE MÖISCEN\n",
      "Prediction (multilingual): hso mafsomot o oney ihont withcrawt wontime aand douringd tir hayfre o aeat  mv\n",
      "Reference: WHATS THE MOST AMOUNT OF MONEY I CAN WITHDRAW AT ONE TIME AND DURING AN ENTIRE DAY FROM AN ATM MACHINE\n",
      "\n",
      "Predicted class: ru, true class: ru.\n",
      "Prediction (single language): КАК НЕТ КРИТВОВ МЕСТНЫЙ СЧЁТ  СПАРАТНЕРОМ\n",
      "Prediction (multilingual): као мнедкритомесный счёт спртняро\n",
      "Reference: КАК МНЕ ОТКРЫТЬ СОВМЕСТНЫЙ СЧЁТ С ПАРТНЕРОМ\n",
      "\n",
      "Predicted class: de, true class: ru.\n",
      "Prediction (single language): BETHUMIER GRTE BERTETALE A WUTEI EFEMLEH KERDTOU MEINER\n",
      "Prediction (multilingual): поче у м карто периь томороботатс еи зоблакировона\n",
      "Reference: ПОЧЕМУ МОЯ КАРТА ПЕРЕСТАЛА РАБОТАТЬ И ЗАБЛОКИРОВАНА\n",
      "\n",
      "Predicted class: ru, true class: ru.\n",
      "Prediction (single language): ДОВРЫЙ ДЕНЬ Я СИВОДНЕ ПОЙТАЛАСА СВАРЖИТЬ ДОВОДНОКРУПНУ ОПЛАТУ ПО ИНТАРНЫТУ ВОТ МЕЯ ПРИХОДИТО ТЕКСТОВО СОЧЕДНЕСМО ЯЕСКАЧТО Я ДОЖН СНЕТ ДЕЛАТ КАОЖИТЬ ПОЖАЛЙСТРО РОМОЧТУ ЯНИНЕ ПОНИМА\n",
      "Prediction (multilingual): доврый день я се бодне пыталусяо сио гешитсеи догол но крп но п лату поинэторнsету  бо мне пли ходестоите кчтова сабще ни с моска штово я оболженуснеделат с кожить пожалсыпру можи я не понима\n",
      "Reference: ДОБРЫЙ ДЕНЬ Я СЕГОДНЯ ПЫТАЛСЯ СОВЕРШИТЬ ДОВОЛЬНО КРУПНУЮ ОПЛАТУ ПО ИНТЕРНЕТУ И ВОТ МНЕ ПРИХОДИТ ТЕКСТОВОЕ СОБЩЕНИЕ ЭСЭМЭСКА ЧТО Я ДОЛЖЕН С НЕЙ ДЕЛАТЬ СКАЖИТЕ ПОЖАЛУЙСТА ПОТОМУ ЧТО Я НЕ ПО НЕ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print_prediction()\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
